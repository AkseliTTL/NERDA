{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('3.9.0')",
   "metadata": {
    "interpreter": {
     "hash": "36071112a161297f2fd106003050184fbdff34ed057f375faa6d2f5f0cad40eb"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Workflow Examples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`NERDA` offers a simple easy-to-use interface for fine-tuning and applying transformer-based models for Named-entity Recognition (='`NERDA` models'). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`NERDA` can be used in two ways. You can either (1) train your own customized `NERDA` model or (2) download and use one of our precooked `NERDA` models for inference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Train Your Own `NERDA` model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We want to fine-tune a transformer for English. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First, we download an English NER dataset [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) with annotated Named Entities, that we will use for training and evaluation of our model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'NERDA'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-920b857cbb44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# don't print warnings for this session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNERDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_dane_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_dane_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdownload_dane_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'NERDA'"
     ]
    }
   ],
   "source": [
    "# don't print warnings for this session\n",
    "from NERDA.datasets import get_conll_data, download_conll_data\n",
    "download_conll_data()"
   ]
  },
  {
   "source": [
    "\n",
    "The DaNE dataset looks like this. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'get_dane_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0e6c3d5c6962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dane_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dane_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tags'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_dane_data' is not defined"
     ]
    }
   ],
   "source": [
    "training = get_conll_data('train', 5)\n",
    "validation = get_conll_data('valid', 5)\n",
    "# example\n",
    "sentence = training.get('sentences')[0]\n",
    "tags = training.get('tags')[0]\n",
    "print(\" \".join([\"{}/{}\".format(word, tag) for word, tag in zip(sentence, tags)]))"
   ]
  },
  {
   "source": [
    "If you provide your own dataset, it must have the same structure (dictionary with 'sentences' and 'tags'), except it does not have to follow the IOB tagging scheme - words that are beginning of entities are tagged with 'B-' and words 'inside' entities are tagged with 'I-')."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Instantiate a `NERDA` model for finetuning an ELECTRA (English) model. Note, this model configuration only uses 5 sentences for model training to minimize execution time. Also the hyperparameters for the model have chosen in order to minimize execution time. This example only serves to illustrate the functionality i.e. the resulting model will suck."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NERDA.models import NERDA\n",
    "model = NERDA(dataset_training = training,\n",
    "              dataset_validation = validation,\n",
    "              transformer = 'google/electra-small-discriminator',\n",
    "              hyperparameters = {'epochs' : 1,\n",
    "                                 'warmup_steps' : 10,\n",
    "                                 'train_batch_size': 5,\n",
    "                                 'learning_rate': 0.0001},)"
   ]
  },
  {
   "source": [
    "By default the network architecture (built on top of the transformer) is analogous to the models in [Hvingelby et al. 2020](http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.565.pdf). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The model can then be trained by invoking the `train` method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "model.train()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "We can then compute the performance on a test set (limited to 5 sentences):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_conll_data('test', 5)\n",
    "model.evaluate_performance(test)"
   ]
  },
  {
   "source": [
    "Unsurprisingly, the model sucks in this case due to the ludicrous specification."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Named Entities in new texts can be predicted with `predict` functions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Old MacDonald had a farm\"\n",
    "model.predict_text(text)"
   ]
  },
  {
   "source": [
    "Needless to say the predicted entities for this particular model are probably nonsensical."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "`NERDA` has more handles, that you can use. You can \n",
    "\n",
    "1. provide your own data set \n",
    "2. choose whatever pretrained transformer you would like to fine-tune\n",
    "3. provide your own set of hyperparameters and lastly\n",
    "4. provide your own `torch` network (architecture)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We have trained a number of more reasonable model configurations, that you can use out-of-the-box. See the chapter below."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Use a Precooked `NERDA` model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We have precooked a number of `NERDA` models, that you can download \n",
    "and use right off the shelf. \n",
    "\n",
    "Here is an example.\n",
    "\n",
    "Instantiate English ELECTRA model, that has been finetuned for NER for English,\n",
    "`EN_ELECTRA_EN`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from NERDA.precooked import EN_ELECTRA_EN\n",
    "model = EN_ELECTRA_EN()\n",
    "\n"
   ]
  },
  {
   "source": [
    "(Down)load network:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.download_network()\n",
    "model.load_network()\n"
   ]
  },
  {
   "source": [
    "This model performs much better:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_performance(get_conll_data('test', 100))"
   ]
  },
  {
   "source": [
    "Predict named entities in new texts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Old MacDonald had a farm'\n",
    "model.predict_text(text)\n"
   ]
  },
  {
   "source": [
    "### List of Precooked Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The table below shows the precooked `NERDA` models publicly available for download. We have trained models for Danish and English.\n",
    "\n",
    "\n",
    "| **Model**       | **Language** | **Transformer**   | **Dataset** | **F1-score** |  \n",
    "|-----------------|--------------|-------------------|---------|-----|\n",
    "| `DA_BERT_ML`    | Danish       | [Multilingual BERT](https://huggingface.co/bert-base-multilingual-uncased) | [DaNE](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane) | xx.x  | \n",
    "`DA_ELECTRA_DA` | Danish       | [Danish ELECTRA](https://huggingface.co/Maltehb/-l-ctra-danish-electra-small-uncased) | [DaNE](https://github.com/alexandrainst/danlp/blob/master/docs/docs/datasets.md#dane) |yy.y             |\n",
    "| `EN_BERT_ML`    | English      | [Multilingual BERT](https://huggingface.co/bert-base-multilingual-uncased)| [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) | zz.z              |\n",
    "| `EN_ELECTRA_EN` | Danish       | [English ELECTRA](https://huggingface.co/google/electra-small-discriminator) | [CoNLL-2003](https://www.clips.uantwerpen.be/conll2003/ner/) | pp.p             |\n",
    "\n",
    "**F1-score** is the micro-averaged F1-score across entity tags and is \n",
    "evaluated on the respective tests (that have not been used for training nor\n",
    "validation of the models).\n",
    "\n",
    "Note, that we have not spent a lot of time on actually fine-tuning the models,\n",
    "so there could be room for improvement. If you are able to improve the models,\n",
    "we will be happy to hear from you and include your `NERDA` model.\n",
    "\n",
    "## Performance (Obsolete)\n",
    "\n",
    "The table below summarizes the performance as measured by F1-scores of the model\n",
    " configurations, that `NERDA` ships with. \n",
    "\n",
    "| **Level**     | **MBERT** | **DABERT** | **ELECTRA** | **XLMROBERTA** | **DISTILMBERT** |\n",
    "|---------------|-----------|------------|-------------|----------------|-----------------|\n",
    "| B-PER         | 0.92      | 0.93       | 0.92        | 0.94           | 0.89            |      \n",
    "| I-PER         | 0.97      | 0.99       | 0.97        | 0.99           | 0.96            |   \n",
    "| B-ORG         | 0.68      | 0.79       | 0.65        | 0.78           | 0.66            |     \n",
    "| I-ORG         | 0.67      | 0.79       | 0.72        | 0.77           | 0.61            |   \n",
    "| B-LOC         | 0.86      | 0.85       | 0.79        | 0.87           | 0.80            |     \n",
    "| I-LOC         | 0.33      | 0.32       | 0.44        | 0.24           | 0.29            |     \n",
    "| B-MISC        | 0.73      | 0.74       | 0.61        | 0.77           | 0.70            |     \n",
    "| I-MISC        | 0.70      | 0.86       | 0.65        | 0.91           | 0.61            |   \n",
    "| **AVG_MICRO** | 0.81      | 0.85       | 0.79        | 0.86           | 0.78            |      \n",
    "| **AVG_MACRO** | 0.73      | 0.78       | 0.72        | 0.78           | 0.69            |"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}